apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: tf
spec:
  failurePolicy:
    maxRestarts: 0              # The set will be restarted on failures up to 0 times.
  successPolicy:
    operator: "All"
    targetReplicatedJobs:
    - "tpu-controller"
  replicatedJobs:
    - name: grpc-worker
      template:
        spec:
          parallelism: 4
          completions: 4
          backoffLimit: 0   # When any pod fails, the job is failed
          template:
            spec:
              nodeSelector:
                cloud.google.com/gke-tpu-accelerator: tpu-v6e-slice
                cloud.google.com/gke-tpu-topology: 4x4
              containers:
              - name: tpu-worker
                image: python:3.10
                env:
                - name: PYTHONUNBUFFERED
                  value: "1"
                # Uncomment to enable PJRT
                - name: NEXT_PLUGGABLE_DEVICE_USE_C_API
                  value: "true"
                - name: TF_PLUGGABLE_DEVICE_LIBRARY_PATH
                  value: "/usr/local/lib/python3.10/site-packages/libtpu/libtpu.so"
                - name: TF_XLA_FLAGS
                  value: "--tf_mlir_enable_mlir_bridge=true --tf_mlir_enable_tpu_variable_runtime_reformatting_pass=false --tf_mlir_enable_convert_control_to_data_outputs_pass=true --tf_mlir_enable_merge_control_flow_pass=true --tf_xla_disable_full_embedding_pipelining=true"
                - name: LIBTPU_INIT_ARGS
                  value: "--xla_sc_splitting_along_feature_dimension=auto --copy_with_dynamic_shape_op_output_pjrt_buffer=true --2a886c8_chip_config_name=megachip_tccontrol"
                ports:
                - containerPort: 8470
                securityContext:
                  privileged: true
                command:
                - bash
                - -c
                - |
                  printenv

                  IFS=',' read -r -a hostnames <<< "$TPU_WORKER_HOSTNAMES"
                  export TPU_HOSTNAME_OVERRIDE="${hostnames[${TPU_WORKER_ID}]}"

                  # Pick one of the tensorflow version
                  # pip install tensorflow-tpu==2.18.0 -f https://storage.googleapis.com/libtpu-tf-releases/index.html --force
                  # pip install https://storage.googleapis.com/tensorflow-public-build-artifacts/prod/tensorflow/official/release/nightly/linux_x86_tpu/wheel_py310/749/20240915-062017/github/tensorflow/build_output/tf_nightly_tpu-2.18.0.dev20240915-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl -f https://storage.googleapis.com/libtpu-tf-releases/index.html --force
                  # pip install tf-nightly-tpu -f https://storage.googleapis.com/libtpu-tf-releases/index.html --force
                  pip install https://storage.googleapis.com/tensorflow-public-build-artifacts/prod/tensorflow/official/release/nightly/linux_x86_tpu/wheel_py310/749/20240915-062017/github/tensorflow/build_output/tf_nightly_tpu-2.18.0.dev20240915-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl -f https://storage.googleapis.com/libtpu-tf-releases/index.html --force


                  # Start GRPC server like grpc_tpu_worker.py
                  python -c "
                  from tensorflow.core.protobuf import config_pb2
                  from tensorflow.core.protobuf import tensorflow_server_pb2
                  from tensorflow.python.training import server_lib

                  server_def = tensorflow_server_pb2.ServerDef(protocol='grpc')
                  job_def = server_def.cluster.job.add()
                  job_def.name = 'tpu_worker'
                  job_def.tasks[0] = 'localhost:8470'
                  server_def.job_name = 'tpu_worker'
                  server_def.task_index = 0

                  config = config_pb2.ConfigProto()

                  # Create GRPC Server instance
                  server = server_lib.Server(server_def, config=config)

                  # join() is blocking, unlike start()
                  server.join()
                  "
                  # Keep container alive for debugging
                  sleep infinity
                resources:
                  limits:
                    google.com/tpu: 4
    - name: tpu-controller
      template:
        spec:
          parallelism: 1
          completions: 1
          backoffLimit: 0   # When any pod fails, the job is failed
          template:
            spec:
              containers:
              - name: tpu-controller
                image: python:3.10
                env:
                - name: PYTHONUNBUFFERED
                  value: "1"
                - name: GRPC_WORKER_NAME # Name of the GRPC worker replicaredJob
                  value: "grpc-worker"
                - name: NUM_REPLICAS   # Number of TPU VM replicas to communicate to
                  value: "4"
                - name: TPU_LOAD_LIBRARY
                  value: "0"
                - name: JOBSET_NAME # Name of the JobSet (`tf`)
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.annotations['jobset.sigs.k8s.io/jobset-name']
                - name: TF_XLA_FLAGS
                  value: "--tf_mlir_enable_mlir_bridge=true --tf_mlir_enable_tpu_variable_runtime_reformatting_pass=false --tf_mlir_enable_convert_control_to_data_outputs_pass=true --tf_mlir_enable_merge_control_flow_pass=true --tf_xla_disable_full_embedding_pipelining=true"
                - name: LIBTPU_INIT_ARGS
                  value: "--xla_sc_splitting_along_feature_dimension=auto --copy_with_dynamic_shape_op_output_pjrt_buffer=true,--2a886c8_chip_config_name=megachip_tccontrol"
                command:
                - bash
                - -c
                - |
                  git clone https://github.com/tensorflow/recommenders.git
                  git clone https://github.com/tensorflow/models.git
                  export PYTHONPATH=/recommenders/:/models/
                  export TF_XLA_FLAGS='--tf_mlir_enable_mlir_bridge=true --tf_xla_sparse_core_disable_table_stacking=true --tf_mlir_enable_convert_control_to_data_outputs_pass=true --tf_mlir_enable_merge_control_flow_pass=true'
                  pip install pyyaml && pip install gin-config && pip install tensorflow-datasets && pip install tf-keras-nightly --no-deps

                  # Pick one of the tensorflow version
                  # pip install tensorflow-tpu==2.18.0 -f https://storage.googleapis.com/libtpu-tf-releases/index.html --force
                  pip install https://storage.googleapis.com/tensorflow-public-build-artifacts/prod/tensorflow/official/release/nightly/linux_x86_tpu/wheel_py310/749/20240915-062017/github/tensorflow/build_output/tf_nightly_tpu-2.18.0.dev20240915-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl -f https://storage.googleapis.com/libtpu-tf-releases/index.html --force
                  # pip install tf-nightly-tpu -f https://storage.googleapis.com/libtpu-tf-releases/index.html --force
                  # pip install https://storage.googleapis.com/tensorflow-public-build-artifacts/prod/tensorflow/official/release/nightly/linux_x86_tpu/wheel_py310/749/20240915-062017/github/tensorflow/build_output/tf_nightly_tpu-2.18.0.dev20240915-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl -f https://storage.googleapis.com/libtpu-tf-releases/index.html --force

                  TF_USE_LEGACY_KERAS=1 TPU_LOAD_LIBRARY=0 python3 ./models/official/recommendation/ranking/train.py  --mode=train     --model_dir=/tmp --params_override="
                  runtime:
                    distribution_strategy: tpu
                    mixed_precision_dtype: 'mixed_bfloat16'
                    tpu: 'grpc://tf-grpc-worker-0-0.tf:8470' # need to pass in grpc endpoints here in GKE
                  task:
                    use_synthetic_data: false
                    use_tf_record_reader: true
                    train_data:
                      input_path: 'gs://trillium-datasets/criteo/train/day_*/*'
                      global_batch_size: 16384
                    validation_data:
                      input_path: 'gs://trillium-datasets/criteo/eval/day_*/*'
                      global_batch_size: 16384
                    model:
                      num_dense_features: 13
                      bottom_mlp: [512, 256, 128]
                      embedding_dim: 128
                      interaction: 'multi_layer_dcn'
                      dcn_num_layers: 3
                      dcn_low_rank_dim: 512
                      size_threshold: 8000
                      top_mlp: [1024, 1024, 512, 256, 1]
                      use_multi_hot: true
                      concat_dense: false
                      dcn_use_bias: true
                      vocab_sizes: [40000000,39060,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36]
                      multi_hot_sizes: [3,2,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1]
                      max_ids_per_chip_per_sample: 128
                      max_ids_per_table: 2048
                      max_unique_ids_per_table: 512
                      use_partial_tpu_embedding: false
                      size_threshold: 0
                      initialize_tables_on_host: true
                  trainer:
                    train_steps: 10000
                    validation_interval: 1000
                    validation_steps: 660
                    summary_interval: 1000
                    steps_per_loop: 1000
                    checkpoint_interval: 0
                    optimizer_config:
                      embedding_optimizer: 'Adagrad'
                      dense_optimizer: 'Adagrad'
                      lr_config:
                        decay_exp: 2
                        decay_start_steps: 70000
                        decay_steps: 30000
                        learning_rate: 0.025
                        warmup_steps: 0
                      dense_sgd_config:
                        decay_exp: 2
                        decay_start_steps: 70000
                        decay_steps: 30000
                        learning_rate: 0.00025
                        warmup_steps: 8000
                    train_tf_function: true
                    train_tf_while_loop: true
                    eval_tf_while_loop: true
                    use_orbit: true
                    pipeline_sparse_and_dense_execution: true"


                  # Do some basic math on each core
                  # python -c "
                  # import os
                  # import tensorflow as tf

                  # endpoint = []
                  # # Building a set of endpoints where the GRPC workers are running
                  # for i in range(int(os.environ['NUM_REPLICAS'])):
                  #   endpoint.append('grpc://{0}-{1}-0-{2}.{0}:8470'.format(os.environ['JOBSET_NAME'], os.environ['GRPC_WORKER_NAME'], i))

                  # endpoint = ','.join(endpoint)

                  # print(endpoint)

                  # cr = tf.distribute.cluster_resolver.TPUClusterResolver(endpoint)
                  # tf.config.experimental_connect_to_cluster(cr)
                  # tf.tpu.experimental.initialize_tpu_system(cr)
                  # strategy = tf.distribute.TPUStrategy(cr)

                  # @tf.function
                  # def add_fn(x,y):
                  #     z = x + y
                  #     return z

                  # x = tf.constant(1.)
                  # y = tf.constant(1.)
                  # z = strategy.run(add_fn, args=(x,y))
                  # print('\nGOT RESULT\n')
                  # print(z)
                  # "

                  # Keep container alive for debugging
             