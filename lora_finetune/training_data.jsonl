{"text": "### Human: What is the main objective of the 'Serve an LLM with GKE Inference Gateway' tutorial? ### Assistant: The tutorial describes how to deploy a large language model (LLM) on Google Kubernetes Engine (GKE) with the GKE Inference Gateway, including steps for cluster setup, model deployment, GKE Inference Gateway configuration, and handling LLM requests."}
{"text": "### Human: What is vLLM and what are its key features? ### Assistant: vLLM is a highly optimized open source LLM serving framework that increases serving throughput on GPUs. Its features include an optimized transformer implementation with PagedAttention, continuous batching for improved throughput, and tensor parallelism for distributed serving across multiple GPUs."}
{"text": "### Human: What are the benefits of using GKE Inference Gateway for serving LLMs? ### Assistant: GKE Inference Gateway enhances GKE for serving LLMs by providing inference-optimized load balancing based on load metrics, support for dense multi-workload serving of LoRA adapters, and model-aware routing for simplified operations."}
{"text": "### Human: What are the prerequisites before starting the GKE Inference Gateway tutorial? ### Assistant: Before you begin, you need a Google Cloud project with billing enabled, the necessary APIs enabled, the roles/container.admin and roles/iam.serviceAccountAdmin, a Hugging Face account, and sufficient quota for H100 GPUs."}
{"text": "### Human: How do I get access to the Llama 3.1 model for the tutorial? ### Assistant: To deploy the Llama 3.1 model, you must first sign the license consent agreement on the model's Hugging Face page and then generate a Hugging Face access token with at least 'Read' permissions from your profile settings."}
{"text": "### Human: What commands should I run in Cloud Shell to set up the default environment variables? ### Assistant: In Cloud Shell, you should run the following commands to set up your environment:\n```bash\ngcloud config set project PROJECT_ID\ngcloud config set billing/quota_project PROJECT_ID\nexport PROJECT_ID=$(gcloud config get project)\nexport REGION=REGION\nexport CLUSTER_NAME=CLUSTER_NAME\nexport HF_TOKEN=HF_TOKEN\n```\nYou need to replace PROJECT_ID, REGION, CLUSTER_NAME, and HF_TOKEN with your specific values."}
{"text": "### Human: Provide the gcloud command to create a GKE Standard cluster suitable for the Inference Gateway tutorial. ### Assistant: To create a Standard GKE cluster, run the following command:\n```bash\ngcloud container clusters create CLUSTER_NAME \\\n    --project=PROJECT_ID \\\n    --region=REGION \\\n    --workload-pool=PROJECT_ID.svc.id.goog \\\n    --release-channel=rapid \\\n    --num-nodes=1 \\\n    --enable-managed-prometheus \\\n    --monitoring=SYSTEM,DCGM\n```"}
{"text": "### Human: How do I create a node pool with H100 GPUs for the Llama 3.1 model? ### Assistant: To create a node pool with two H100 GPUs, run this gcloud command:\n```bash\ngcloud container node-pools create gpupool \\\n    --accelerator type=nvidia-h100-80gb,count=2,gpu-driver-version=latest \\\n    --project=PROJECT_ID \\\n    --location=REGION \\\n    --node-locations=REGION-a \\\n    --cluster=CLUSTER_NAME \\\n    --machine-type=a3-highgpu-2g \\\n    --num-nodes=1 \\\n    --disk-type=\"pd-standard\"\n```"}
{"text": "### Human: How do I create a Kubernetes secret for my Hugging Face token? ### Assistant: First, get credentials for your cluster using `gcloud container clusters get-credentials CLUSTER_NAME --location=REGION`. Then, create the secret with this command, replacing HF_TOKEN with your actual token and HF_SECRET with a name like 'hf-secret':\n```bash\nkubectl create secret generic HF_SECRET \\\n      --from-literal=token=HF_TOKEN \\\n      --dry-run=client -o yaml | kubectl apply -f -\n```"}
{"text": "### Human: What command installs the necessary CRDs for GKE Inference Gateway? ### Assistant: To install the InferencePool and InferenceModel CRDs in your GKE cluster, run the following kubectl command:\n```bash\nkubectl apply -f [https://github.com/kubernetes-sigs/gateway-api-inference-extension/releases/download/v0.3.0/manifests.yaml](https://github.com/kubernetes-sigs/gateway-api-inference-extension/releases/download/v0.3.0/manifests.yaml)\n```"}
{"text": "### Human: Show me the full YAML manifest to deploy a vLLM model server for Llama-3.1-8B-Instruct with LoRA adapters. ### Assistant: Here is the complete manifest for the Kubernetes Deployment, ConfigMap, and HealthCheckPolicy. Save it as `vllm-llama3-8b-instruct.yaml` and apply it with `kubectl apply -f vllm-llama3-8b-instruct.yaml`.\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vllm-llama3-8b-instruct\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: vllm-llama3-8b-instruct\n  template:\n    metadata:\n      labels:\n        app: vllm-llama3-8b-instruct\n    spec:\n      containers:\n        - name: vllm\n          image: \"vllm/vllm-openai:latest\"\n          imagePullPolicy: Always\n          command: [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n          args:\n          - \"--model\"\n          - \"meta-llama/Llama-3.1-8B-Instruct\"\n          - \"--tensor-parallel-size\"\n          - \"1\"\n          - \"--port\"\n          - \"8000\"\n          - \"--enable-lora\"\n          - \"--max-loras\"\n          - \"2\"\n          - \"--max-cpu-loras\"\n          - \"12\"\n          env:\n            - name: VLLM_USE_V1\n              value: \"1\"\n            - name: PORT\n              value: \"8000\"\n            - name: HUGGING_FACE_HUB_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: hf-token\n                  key: token\n            - name: VLLM_ALLOW_RUNTIME_LORA_UPDATING\n              value: \"true\"\n          ports:\n            - containerPort: 8000\n              name: http\n              protocol: TCP\n          lifecycle:\n            preStop:\n              sleep:\n                seconds: 30\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n            periodSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            timeoutSeconds: 1\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n            periodSeconds: 1\n            successThreshold: 1\n            failureThreshold: 1\n            timeoutSeconds: 1\n          startupProbe:\n            failureThreshold: 3600\n            initialDelaySeconds: 2\n            periodSeconds: 1\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n          resources:\n            limits:\n              [nvidia.com/gpu](https://nvidia.com/gpu): 1\n            requests:\n              [nvidia.com/gpu](https://nvidia.com/gpu): 1\n          volumeMounts:\n            - mountPath: /data\n              name: data\n            - mountPath: /dev/shm\n              name: shm\n            - name: adapters\n              mountPath: \"/adapters\"\n      initContainers:\n        - name: lora-adapter-syncer\n          tty: true\n          stdin: true\n          image: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension/lora-syncer:main\n          restartPolicy: Always\n          imagePullPolicy: Always\n          env:\n            - name: DYNAMIC_LORA_ROLLOUT_CONFIG\n              value: \"/config/configmap.yaml\"\n          volumeMounts:\n          - name: config-volume\n            mountPath:  /config\n      restartPolicy: Always\n      enableServiceLinks: false\n      terminationGracePeriodSeconds: 130\n      nodeSelector:\n        [cloud.google.com/gke-accelerator](https://cloud.google.com/gke-accelerator): \"nvidia-h100-80gb\"\n      volumes:\n        - name: data\n          emptyDir: {}\n        - name: shm\n          emptyDir:\n            medium: Memory\n        - name: adapters\n          emptyDir: {}\n        - name: config-volume\n          configMap:\n            name: vllm-llama3-8b-adapters\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vllm-llama3-8b-adapters\ndata:\n  configmap.yaml: |\n      vLLMLoRAConfig:\n        name: vllm-llama3.1-8b-instruct\n        port: 8000\n        defaultBaseModel: meta-llama/Llama-3.1-8B-Instruct\n        ensureExist:\n          models:\n          - id: food-review\n            source: Kawon/llama3.1-food-finetune_v14_r8\n          - id: cad-fabricator\n            source: redcathode/fabricator\n---\nkind: HealthCheckPolicy\napiVersion: networking.gke.io/v1\nmetadata:\n  name: health-check-policy\n  namespace: default\nspec:\n  targetRef:\n    group: \"inference.networking.x-k8s.io\"\n    kind: InferencePool\n    name: vllm-llama3-8b-instruct\n  default:\n    config:\n      type: HTTP\n      httpHealthCheck:\n          requestPath: /health\n          port: 8000\n```"}
{"text": "### Human: What is an InferencePool resource and how do I create one? ### Assistant: An InferencePool is a Kubernetes custom resource that defines a group of Pods with a common base LLM and compute configuration. It allows the GKE Inference Gateway to route traffic to your model server Pods. You can create one using this Helm command, making sure to match the label selector to your deployment:\n```bash\nhelm install vllm-llama3-8b-instruct \\\n  --set inferencePool.modelServers.matchLabels.app=vllm-llama3-8b-instruct \\\n  --set provider.name=gke \\\n  --set healthCheckPolicy.create=false \\\n  --version v0.3.0 \\\n  oci://registry.k8s.io/gateway-api-inference-extension/charts/inferencepool\n```"}
{"text": "### Human: What is an InferenceModel resource and what are its key fields? ### Assistant: The InferenceModel Kubernetes custom resource defines a specific model, including LoRA-tuned models, and its serving criticality. Its key fields are `modelName`, `criticality` (Critical, Standard, or Sheddable), and `poolRef` which references the InferencePool."}
{"text": "### Human: Show me a YAML example for creating an InferenceModel resource for a LoRA adapter and a base model. ### Assistant: Here is an example that creates an InferenceModel for the 'food-review' LoRA adapter with 'Standard' criticality, and another for the base model 'meta-llama/Llama-3.1-8B-Instruct' with 'Critical' criticality, both on the 'vllm-llama3-8b-instruct' pool.\n```yaml\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: food-review\nspec:\n  modelName: food-review\n  criticality: Standard\n  poolRef:\n    name: vllm-llama3-8b-instruct\n  targetModels:\n  - name: food-review\n    weight: 100\n---\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: llama3-base-model\nspec:\n  modelName: meta-llama/Llama-3.1-8B-Instruct\n  criticality: Critical\n  poolRef:\n    name: vllm-llama3-8b-instruct\n```"}
{"text": "### Human: How do I create the Gateway resource to expose my inference service? ### Assistant: Save the following manifest as `gateway.yaml` and apply it with `kubectl apply -f gateway.yaml`. This creates a Gateway that acts as the entry point for external traffic.\n```yaml\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: GATEWAY_NAME\nspec:\n  gatewayClassName: gke-l7-regional-external-managed\n  listeners:\n    - protocol: HTTP\n      port: 80\n      name: http\n```\nRemember to replace GATEWAY_NAME with a unique name like 'inference-gateway'."}
{"text": "### Human: What is an HTTPRoute and how do I create one to route traffic to my InferencePool? ### Assistant: An HTTPRoute resource defines how the Gateway routes incoming HTTP requests to backend services like your InferencePool. You can create one by saving this manifest as `httproute.yaml` and applying it.\n```yaml\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: HTTPROUTE_NAME\nspec:\n  parentRefs:\n  - name: GATEWAY_NAME\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: PATH_PREFIX\n    backendRefs:\n    - name: INFERENCE_POOL_NAME\n      group: inference.networking.x-k8s.io\n      kind: InferencePool\n```\nReplace HTTPROUTE_NAME, GATEWAY_NAME, PATH_PREFIX, and INFERENCE_POOL_NAME with your specific configuration values."}
{"text": "### Human: How do I send an inference request to my deployed model using curl? ### Assistant: First, get the Gateway endpoint IP and Port. Then, use the following curl command to send a request to the `/v1/completions` endpoint, replacing the placeholder values as needed:\n```bash\n# Get the Gateway IP and Port\nIP=$(kubectl get gateway/GATEWAY_NAME -o jsonpath='{.status.addresses[0].value}')\nPORT=80 # Or 443 for HTTPS\n\n# Send the request\ncurl -i -X POST https://${IP}:${PORT}/v1/completions \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer $(gcloud auth print-access-token)' \\\n-d '{\n    \"model\": \"MODEL_NAME\",\n    \"prompt\": \"PROMPT_TEXT\",\n    \"max_tokens\": MAX_TOKENS,\n    \"temperature\": \"TEMPERATURE\"\n}'\n```"}
{"text": "### Human: What does the `terminationGracePeriodSeconds` field in the deployment manifest control? ### Assistant: The `terminationGracePeriodSeconds` controls how long Kubernetes waits before forcibly terminating a pod after it receives a SIGTERM signal. This period should be long enough to allow the server to finish processing in-flight requests and for the load balancer to remove the pod from its rotation, which is why it's set to 130 seconds in the example to provide a safety buffer."}
{"text": "### Human: What is the purpose of the `startupProbe` in the vLLM deployment? ### Assistant: The `startupProbe` is used to determine if the container has started successfully. Kubernetes will not check the `livenessProbe` or `readinessProbe` until the `startupProbe` succeeds. This is crucial for LLM deployments because model loading can take a very long time, and this probe prevents the pod from being killed prematurely before the model is fully loaded and ready to serve traffic."}
{"text": "### Human: How do I clean up the resources created in this tutorial to avoid incurring charges? ### Assistant: To avoid charges, delete the GKE cluster by running the following command, replacing CLUSTER_NAME and REGION with your values:\n```bash\ngcloud container clusters delete CLUSTER_NAME \\\n    --region=REGION\n```"}
{"text": "### Human: What are the prerequisites for the Inference Gateway quickstart guide? ### Assistant: The quickstart guide requires a Kubernetes cluster with support for services of type LoadBalancer and support for sidecar containers, which is enabled by default since Kubernetes v1.29."}
{"text": "### Human: What model server options are presented in the quickstart guide? ### Assistant: The guide presents three options for the model server: a GPU-based server using meta-llama/Llama-3.1-8B-Instruct, a CPU-based server using Qwen/Qwen2.5-1.5B-Instruct, and a vLLM Simulator server that simulates the Llama model without using GPUs."}
{"text": "### Human: How do I deploy the GPU-based model server from the quickstart guide? ### Assistant: First, you must create a Hugging Face secret using `kubectl create secret generic hf-token --from-literal=token=$HF_TOKEN`. After that, deploy the model server by applying the manifest from the main branch: `kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/gpu-deployment.yaml`."}
{"text": "### Human: How can I install the latest release of the Inference Extension CRDs? ### Assistant: You can install the latest Custom Resource Definitions for the extension by running the command: `kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/releases/latest/download/manifests.yaml`."}
{"text": "### Human: What command deploys the sample InferencePool and Endpoint Picker Extension? ### Assistant: You can deploy the InferencePool and the Endpoint Picker Extension together by applying a single manifest: `kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/inferencepool-resources.yaml`."}
{"text": "### Human: How do I check if my Inference Gateway has been deployed successfully on GKE? ### Assistant: Run the command `kubectl get gateway inference-gateway`. A successful deployment is indicated by the Gateway having an IP address assigned under the ADDRESS column and showing `True` in the PROGRAMMED status column."}
{"text": "### Human: Why should I apply a GCPBackendPolicy and what is the command to do it? ### Assistant: It is recommended to apply a GCPBackendPolicy because the default connection timeout may be too short for many inference workloads. This policy lets you set a more appropriate timeout. Apply it using the command: `kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/gcp-backend-policy.yaml`."}
{"text": "### Human: How do I test the Inference Gateway from the quickstart guide using curl? ### Assistant: First, get the Gateway's external IP address and port. Then, send a POST request with your prompt. The full command sequence is:\n```bash\nIP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}')\nPORT=80\ncurl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\"model\": \"food-review\",\"prompt\": \"Write as if you were a critic: San Francisco\",\"max_tokens\": 100,\"temperature\": 0}'\n```"}
{"text": "### Human: How do I clean up all resources created by the quickstart guide? ### Assistant: To clean up all resources, you should run a series of `kubectl delete` commands using the `-f` flag for each manifest applied during setup (such as `inferencepool-resources.yaml` and `gpu-deployment.yaml`) and the `-k` flag for the CRDs (`.../config/crd`). Use the `--ignore-not-found` flag to prevent errors if a resource was already deleted. Finally, delete the `hf-token` secret."}
