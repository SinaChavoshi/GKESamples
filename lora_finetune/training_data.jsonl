{"text": "### Human: What is the main objective of the 'Serve an LLM with GKE Inference Gateway' tutorial? ### Assistant: The tutorial describes how to deploy a large language model (LLM) on Google Kubernetes Engine (GKE) with the GKE Inference Gateway, including steps for cluster setup, model deployment, GKE Inference Gateway configuration, and handling LLM requests."}
{"text": "### Human: What is vLLM and what are its key features? ### Assistant: vLLM is a highly optimized open source LLM serving framework that increases serving throughput on GPUs. Its features include an optimized transformer implementation with PagedAttention, continuous batching for improved throughput, and tensor parallelism for distributed serving across multiple GPUs."}
{"text": "### Human: What are the benefits of using GKE Inference Gateway for serving LLMs? ### Assistant: GKE Inference Gateway enhances GKE for serving LLMs by providing inference-optimized load balancing based on load metrics, support for dense multi-workload serving of LoRA adapters, and model-aware routing for simplified operations."}
{"text": "### Human: What are the prerequisites before starting the GKE Inference Gateway tutorial? ### Assistant: Before you begin, you need a Google Cloud project with billing enabled, the necessary APIs enabled, the roles/container.admin and roles/iam.serviceAccountAdmin, a Hugging Face account, and sufficient quota for H100 GPUs."}
{"text": "### Human: How do I get access to the Llama 3.1 model for the tutorial? ### Assistant: To deploy the Llama 3.1 model, you must first sign the license consent agreement on the model's Hugging Face page and then generate a Hugging Face access token with at least 'Read' permissions from your profile settings."}
{"text": "### Human: What commands should I run in Cloud Shell to set up the default environment variables? ### Assistant: In Cloud Shell, you should run the following commands to set up your environment:\n```bash\ngcloud config set project PROJECT_ID\ngcloud config set billing/quota_project PROJECT_ID\nexport PROJECT_ID=$(gcloud config get project)\nexport REGION=REGION\nexport CLUSTER_NAME=CLUSTER_NAME\nexport HF_TOKEN=HF_TOKEN\n```\nYou need to replace PROJECT_ID, REGION, CLUSTER_NAME, and HF_TOKEN with your specific values."}
{"text": "### Human: Provide the gcloud command to create a GKE Standard cluster suitable for the Inference Gateway tutorial. ### Assistant: To create a Standard GKE cluster, run the following command:\n```bash\ngcloud container clusters create CLUSTER_NAME \\\n    --project=PROJECT_ID \\\n    --region=REGION \\\n    --workload-pool=PROJECT_ID.svc.id.goog \\\n    --release-channel=rapid \\\n    --num-nodes=1 \\\n    --enable-managed-prometheus \\\n    --monitoring=SYSTEM,DCGM\n```"}
{"text": "### Human: How do I create a node pool with H100 GPUs for the Llama 3.1 model? ### Assistant: To create a node pool with two H100 GPUs, run this gcloud command:\n```bash\ngcloud container node-pools create gpupool \\\n    --accelerator type=nvidia-h100-80gb,count=2,gpu-driver-version=latest \\\n    --project=PROJECT_ID \\\n    --location=REGION \\\n    --node-locations=REGION-a \\\n    --cluster=CLUSTER_NAME \\\n    --machine-type=a3-highgpu-2g \\\n    --num-nodes=1 \\\n    --disk-type=\"pd-standard\"\n```"}
{"text": "### Human: How do I create a Kubernetes secret for my Hugging Face token? ### Assistant: First, get credentials for your cluster using `gcloud container clusters get-credentials CLUSTER_NAME --location=REGION`. Then, create the secret with this command, replacing HF_TOKEN with your actual token and HF_SECRET with a name like 'hf-secret':\n```bash\nkubectl create secret generic HF_SECRET \\\n      --from-literal=token=HF_TOKEN \\\n      --dry-run=client -o yaml | kubectl apply -f -\n```"}
{"text": "### Human: What command installs the necessary CRDs for GKE Inference Gateway? ### Assistant: To install the InferencePool and InferenceModel CRDs in your GKE cluster, run the following kubectl command:\n```bash\nkubectl apply -f [https://github.com/kubernetes-sigs/gateway-api-inference-extension/releases/download/v0.3.0/manifests.yaml](https://github.com/kubernetes-sigs/gateway-api-inference-extension/releases/download/v0.3.0/manifests.yaml)\n```"}
{"text": "### Human: Show me the full YAML manifest to deploy a vLLM model server for Llama-3.1-8B-Instruct with LoRA adapters. ### Assistant: Here is the complete manifest for the Kubernetes Deployment, ConfigMap, and HealthCheckPolicy. Save it as `vllm-llama3-8b-instruct.yaml` and apply it with `kubectl apply -f vllm-llama3-8b-instruct.yaml`.\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vllm-llama3-8b-instruct\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: vllm-llama3-8b-instruct\n  template:\n    metadata:\n      labels:\n        app: vllm-llama3-8b-instruct\n    spec:\n      containers:\n        - name: vllm\n          image: \"vllm/vllm-openai:latest\"\n          imagePullPolicy: Always\n          command: [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n          args:\n          - \"--model\"\n          - \"meta-llama/Llama-3.1-8B-Instruct\"\n          - \"--tensor-parallel-size\"\n          - \"1\"\n          - \"--port\"\n          - \"8000\"\n          - \"--enable-lora\"\n          - \"--max-loras\"\n          - \"2\"\n          - \"--max-cpu-loras\"\n          - \"12\"\n          env:\n            - name: VLLM_USE_V1\n              value: \"1\"\n            - name: PORT\n              value: \"8000\"\n            - name: HUGGING_FACE_HUB_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: hf-token\n                  key: token\n            - name: VLLM_ALLOW_RUNTIME_LORA_UPDATING\n              value: \"true\"\n          ports:\n            - containerPort: 8000\n              name: http\n              protocol: TCP\n          lifecycle:\n            preStop:\n              sleep:\n                seconds: 30\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n            periodSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            timeoutSeconds: 1\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n            periodSeconds: 1\n            successThreshold: 1\n            failureThreshold: 1\n            timeoutSeconds: 1\n          startupProbe:\n            failureThreshold: 3600\n            initialDelaySeconds: 2\n            periodSeconds: 1\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n          resources:\n            limits:\n              [nvidia.com/gpu](https://nvidia.com/gpu): 1\n            requests:\n              [nvidia.com/gpu](https://nvidia.com/gpu): 1\n          volumeMounts:\n            - mountPath: /data\n              name: data\n            - mountPath: /dev/shm\n              name: shm\n            - name: adapters\n              mountPath: \"/adapters\"\n      initContainers:\n        - name: lora-adapter-syncer\n          tty: true\n          stdin: true\n          image: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension/lora-syncer:main\n          restartPolicy: Always\n          imagePullPolicy: Always\n          env:\n            - name: DYNAMIC_LORA_ROLLOUT_CONFIG\n              value: \"/config/configmap.yaml\"\n          volumeMounts:\n          - name: config-volume\n            mountPath:  /config\n      restartPolicy: Always\n      enableServiceLinks: false\n      terminationGracePeriodSeconds: 130\n      nodeSelector:\n        [cloud.google.com/gke-accelerator](https://cloud.google.com/gke-accelerator): \"nvidia-h100-80gb\"\n      volumes:\n        - name: data\n          emptyDir: {}\n        - name: shm\n          emptyDir:\n            medium: Memory\n        - name: adapters\n          emptyDir: {}\n        - name: config-volume\n          configMap:\n            name: vllm-llama3-8b-adapters\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vllm-llama3-8b-adapters\ndata:\n  configmap.yaml: |\n      vLLMLoRAConfig:\n        name: vllm-llama3.1-8b-instruct\n        port: 8000\n        defaultBaseModel: meta-llama/Llama-3.1-8B-Instruct\n        ensureExist:\n          models:\n          - id: food-review\n            source: Kawon/llama3.1-food-finetune_v14_r8\n          - id: cad-fabricator\n            source: redcathode/fabricator\n---\nkind: HealthCheckPolicy\napiVersion: networking.gke.io/v1\nmetadata:\n  name: health-check-policy\n  namespace: default\nspec:\n  targetRef:\n    group: \"inference.networking.x-k8s.io\"\n    kind: InferencePool\n    name: vllm-llama3-8b-instruct\n  default:\n    config:\n      type: HTTP\n      httpHealthCheck:\n          requestPath: /health\n          port: 8000\n```"}
{"text": "### Human: What is an InferencePool resource and how do I create one? ### Assistant: An InferencePool is a Kubernetes custom resource that defines a group of Pods with a common base LLM and compute configuration. It allows the GKE Inference Gateway to route traffic to your model server Pods. You can create one using this Helm command, making sure to match the label selector to your deployment:\n```bash\nhelm install vllm-llama3-8b-instruct \\\n  --set inferencePool.modelServers.matchLabels.app=vllm-llama3-8b-instruct \\\n  --set provider.name=gke \\\n  --set healthCheckPolicy.create=false \\\n  --version v0.3.0 \\\n  oci://registry.k8s.io/gateway-api-inference-extension/charts/inferencepool\n```"}
{"text": "### Human: What is an InferenceModel resource and what are its key fields? ### Assistant: The InferenceModel Kubernetes custom resource defines a specific model, including LoRA-tuned models, and its serving criticality. Its key fields are `modelName`, `criticality` (Critical, Standard, or Sheddable), and `poolRef` which references the InferencePool."}
{"text": "### Human: Show me a YAML example for creating an InferenceModel resource for a LoRA adapter and a base model. ### Assistant: Here is an example that creates an InferenceModel for the 'food-review' LoRA adapter with 'Standard' criticality, and another for the base model 'meta-llama/Llama-3.1-8B-Instruct' with 'Critical' criticality, both on the 'vllm-llama3-8b-instruct' pool.\n```yaml\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: food-review\nspec:\n  modelName: food-review\n  criticality: Standard\n  poolRef:\n    name: vllm-llama3-8b-instruct\n  targetModels:\n  - name: food-review\n    weight: 100\n---\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: llama3-base-model\nspec:\n  modelName: meta-llama/Llama-3.1-8B-Instruct\n  criticality: Critical\n  poolRef:\n    name: vllm-llama3-8b-instruct\n```"}
{"text": "### Human: How do I create the Gateway resource to expose my inference service? ### Assistant: Save the following manifest as `gateway.yaml` and apply it with `kubectl apply -f gateway.yaml`. This creates a Gateway that acts as the entry point for external traffic.\n```yaml\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: GATEWAY_NAME\nspec:\n  gatewayClassName: gke-l7-regional-external-managed\n  listeners:\n    - protocol: HTTP\n      port: 80\n      name: http\n```\nRemember to replace GATEWAY_NAME with a unique name like 'inference-gateway'."}
{"text": "### Human: What is an HTTPRoute and how do I create one to route traffic to my InferencePool? ### Assistant: An HTTPRoute resource defines how the Gateway routes incoming HTTP requests to backend services like your InferencePool. You can create one by saving this manifest as `httproute.yaml` and applying it.\n```yaml\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: HTTPROUTE_NAME\nspec:\n  parentRefs:\n  - name: GATEWAY_NAME\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: PATH_PREFIX\n    backendRefs:\n    - name: INFERENCE_POOL_NAME\n      group: inference.networking.x-k8s.io\n      kind: InferencePool\n```\nReplace HTTPROUTE_NAME, GATEWAY_NAME, PATH_PREFIX, and INFERENCE_POOL_NAME with your specific configuration values."}
{"text": "### Human: How do I send an inference request to my deployed model using curl? ### Assistant: First, get the Gateway endpoint IP and Port. Then, use the following curl command to send a request to the `/v1/completions` endpoint, replacing the placeholder values as needed:\n```bash\n# Get the Gateway IP and Port\nIP=$(kubectl get gateway/GATEWAY_NAME -o jsonpath='{.status.addresses[0].value}')\nPORT=80 # Or 443 for HTTPS\n\n# Send the request\ncurl -i -X POST https://${IP}:${PORT}/v1/completions \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer $(gcloud auth print-access-token)' \\\n-d '{\n    \"model\": \"MODEL_NAME\",\n    \"prompt\": \"PROMPT_TEXT\",\n    \"max_tokens\": MAX_TOKENS,\n    \"temperature\": \"TEMPERATURE\"\n}'\n```"}
{"text": "### Human: What does the `terminationGracePeriodSeconds` field in the deployment manifest control? ### Assistant: The `terminationGracePeriodSeconds` controls how long Kubernetes waits before forcibly terminating a pod after it receives a SIGTERM signal. This period should be long enough to allow the server to finish processing in-flight requests and for the load balancer to remove the pod from its rotation, which is why it's set to 130 seconds in the example to provide a safety buffer."}
{"text": "### Human: What is the purpose of the `startupProbe` in the vLLM deployment? ### Assistant: The `startupProbe` is used to determine if the container has started successfully. Kubernetes will not check the `livenessProbe` or `readinessProbe` until the `startupProbe` succeeds. This is crucial for LLM deployments because model loading can take a very long time, and this probe prevents the pod from being killed prematurely before the model is fully loaded and ready to serve traffic."}
{"text": "### Human: How do I clean up the resources created in this tutorial to avoid incurring charges? ### Assistant: To avoid charges, delete the GKE cluster by running the following command, replacing CLUSTER_NAME and REGION with your values:\n```bash\ngcloud container clusters delete CLUSTER_NAME \\\n    --region=REGION\n```"}
{"text": "### Human: What are the prerequisites for the Inference Gateway quickstart guide? ### Assistant: The quickstart guide requires a Kubernetes cluster with support for services of type LoadBalancer and support for sidecar containers, which is enabled by default since Kubernetes v1.29."}
{"text": "### Human: What model server options are presented in the quickstart guide? ### Assistant: The guide presents three options for the model server: a GPU-based server using meta-llama/Llama-3.1-8B-Instruct, a CPU-based server using Qwen/Qwen2.5-1.5B-Instruct, and a vLLM Simulator server that simulates the Llama model without using GPUs."}
{"text": "### Human: How do I deploy the GPU-based model server from the quickstart guide? ### Assistant: First, you must create a Hugging Face secret using `kubectl create secret generic hf-token --from-literal=token=$HF_TOKEN`. After that, deploy the model server by applying the manifest from the main branch: `kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/gpu-deployment.yaml`."}
{"text": "### Human: How can I install the latest release of the Inference Extension CRDs? ### Assistant: You can install the latest Custom Resource Definitions for the extension by running the command: `kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/releases/latest/download/manifests.yaml`."}
{"text": "### Human: What command deploys the sample InferencePool and Endpoint Picker Extension? ### Assistant: You can deploy the InferencePool and the Endpoint Picker Extension together by applying a single manifest: `kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/inferencepool-resources.yaml`."}
{"text": "### Human: How do I check if my Inference Gateway has been deployed successfully on GKE? ### Assistant: Run the command `kubectl get gateway inference-gateway`. A successful deployment is indicated by the Gateway having an IP address assigned under the ADDRESS column and showing `True` in the PROGRAMMED status column."}
{"text": "### Human: Why should I apply a GCPBackendPolicy and what is the command to do it? ### Assistant: It is recommended to apply a GCPBackendPolicy because the default connection timeout may be too short for many inference workloads. This policy lets you set a more appropriate timeout. Apply it using the command: `kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/gcp-backend-policy.yaml`."}
{"text": "### Human: How do I test the Inference Gateway from the quickstart guide using curl? ### Assistant: First, get the Gateway's external IP address and port. Then, send a POST request with your prompt. The full command sequence is:\n```bash\nIP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}')\nPORT=80\ncurl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\"model\": \"food-review\",\"prompt\": \"Write as if you were a critic: San Francisco\",\"max_tokens\": 100,\"temperature\": 0}'\n```"}
{"text": "### Human: How do I clean up all resources created by the quickstart guide? ### Assistant: To clean up all resources, you should run a series of `kubectl delete` commands using the `-f` flag for each manifest applied during setup (such as `inferencepool-resources.yaml` and `gpu-deployment.yaml`) and the `-k` flag for the CRDs (`.../config/crd`). Use the `--ignore-not-found` flag to prevent errors if a resource was already deleted. Finally, delete the `hf-token` secret."}
{"text": "### Human: How does Inference Gateway help efficiently serve multiple LoRA adapters? ### Assistant: Inference Gateway allows you to deploy multiple dynamic LoRA fine-tuned adapters (like 'english-bot' and 'spanish-bot') on a common base model and accelerator pool. This reduces the number of required accelerators by densely packing multiple models into a shared pool, making efficient use of GPU and TPU capacity."}
{"text": "### Human: Show me the YAML for an InferencePool for a shared gemma3 model. ### Assistant: Here is the YAML for an InferencePool resource that selects pods for a shared gemma3 base model:\n```yaml\napiVersion: gateway.networking.x-k8s.io/v1alpha1\nkind: InferencePool\nmetadata:\n  name: gemma3\nspec:\n  selector:\n    pool: gemma3\n```"}
{"text": "### Human: How would I define two LoRA adapters, 'english-bot' and 'spanish-bot', on the same InferencePool with different criticalities? ### Assistant: You would create two separate InferenceModel resources. Each one specifies a unique `modelName` and `criticality`, but they both point to the same `poolRef`. Here is the YAML:\n```yaml\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: english-bot\nspec:\n  modelName: english-bot\n  criticality: Standard\n  poolRef:\n    name: gemma3\n---\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: spanish-bot\nspec:\n  modelName: spanish-bot\n  criticality: Critical\n  poolRef:\n    name: gemma3\n```"}
{"text": "### Human: How do I route traffic from a Gateway to a shared InferencePool named 'gemma3'? ### Assistant: You create an HTTPRoute resource that uses the 'gemma3' InferencePool as its backendRef. This example routes all traffic from the root path ('/') of the 'inference-gateway' to the pool.\n```yaml\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: inference-gateway\nspec:\n  listeners:\n  - protocol: HTTP\n    port: 80\n    name: http\n---\nkind: HTTPRoute\napiVersion: gateway.networking.k8s.io/v1\nmetadata:\n  name: routes-to-llms\nspec:\n  parentRefs:\n    - name: inference-gateway\n  rules:\n  - matches:\n      path:\n        type: PathPrefix\n        value: /\n    backendRefs:\n    - name: gemma3\n      kind: InferencePool\n```"}
{"text": "### Human: Provide the curl command to send a request to the 'english-bot' LoRA adapter. ### Assistant: First, get the gateway IP and port. Then, you can send a request by specifying 'english-bot' as the model in the JSON payload.\n```bash\nIP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}'); PORT=80\ncurl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\"model\": \"english-bot\",\"prompt\": \"What is the color of the sky\",\"max_tokens\": 100,\"temperature\": 0}'\n```"}
{"text": "### Human: How do I test the 'spanish-bot' LoRA model via the Inference Gateway? ### Assistant: Get the gateway IP and port. Then, send a curl request specifying 'spanish-bot' as the model in the request body.\n```bash\nIP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}'); PORT=80\ncurl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\"model\": \"spanish-bot\",\"prompt\": \"¿De qué color es...?\",\"max_tokens\": 100,\"temperature\": 0}'\n```"}
{"text": "### Human: How can Inference Gateway serve multiple different base models, like Gemma3 and Deepseek, for different workloads? ### Assistant: By using an Inference Gateway, you can deploy different LLMs like Gemma3 and Deepseek into separate InferencePools, each with its own accelerator configuration. You can then create an HTTPRoute that uses Body-Based Routing to read the model name from the request body, copy it to a header, and route traffic to the correct InferencePool based on that header value."}
{"text": "### Human: Provide the HTTPRoute YAML for routing requests to a 'chatbot' and a 'recommender' model. ### Assistant: This HTTPRoute manifest uses header matching on 'X-Gateway-Model-Name' to route requests. A request for the 'chatbot' model goes to the 'gemma3' InferencePool, and a request for the 'recommender' model goes to the 'deepseek-r1' InferencePool.\n```yaml\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: routes-to-llms\nspec:\n  parentRefs:\n  - name: inference-gateway\n  rules:\n  - matches:\n    - headers:\n      - type: Exact\n        name: X-Gateway-Model-Name\n        value: chatbot\n      path:\n        type: PathPrefix\n        value: /\n    backendRefs:\n    - name: gemma3\n      kind: InferencePool\n  - matches:\n    - headers:\n      - type: Exact\n        name: X-Gateway-Model-Name\n        value: recommender\n      path:\n        type: PathPrefix\n        value: /\n    backendRefs:\n    - name: deepseek-r1\n      kind: InferencePool\n```"}
{"text": "### Human: How do I send a test request to a 'chatbot' model that is routed by header matching? ### Assistant: Get the gateway IP and port, then send a curl request specifying 'chatbot' as the model in the JSON body. Body-Based Routing will handle converting this to the correct header for routing.\n```bash\nIP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}'); PORT=80\ncurl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\"model\": \"chatbot\",\"prompt\": \"What is the color of the sky\",\"max_tokens\": 100,\"temperature\": 0}'\n```"}
{"text": "### Human: How would I test the 'recommender' model that is served on a separate InferencePool? ### Assistant: You would get the gateway IP and port, and then send a curl request with the 'model' field in the JSON body set to 'recommender'.\n```bash\nIP=$(kubectl get gateway/inference-gateway -o jsonpath='{.status.addresses[0].value}'); PORT=80\ncurl -i ${IP}:${PORT}/v1/completions -H 'Content-Type: application/json' -d '{\"model\": \"recommender\",\"prompt\": \"Give me restaurant recommendations in Paris\",\"max_tokens\": 100,\"temperature\": 0}'\n```"}
{"text": "### Human: How do I create the Service Account and ClusterRoles needed for Prometheus to scrape metrics from the Inference Gateway? ### Assistant: To set up the necessary authorization for metrics scraping, you apply a multi-document YAML that creates a ServiceAccount, ClusterRole, ClusterRoleBinding, and a Secret. You can apply it directly with this command:\n```bash\nkubectl apply -f - <<EOF\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: inference-gateway-metrics-reader\nrules:\n- nonResourceURLs:\n  - /metrics\n  verbs:\n  - get\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: inference-gateway-sa-metrics-reader\n  namespace: default\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: inference-gateway-sa-metrics-reader-role-binding\n  namespace: default\nsubjects:\n- kind: ServiceAccount\n  name: inference-gateway-sa-metrics-reader\n  namespace: default\nroleRef:\n  kind: ClusterRole\n  name: inference-gateway-metrics-reader\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: inference-gateway-sa-metrics-reader-secret\n  namespace: default\n  annotations:\n    kubernetes.io/service-account.name: inference-gateway-sa-metrics-reader\ntype: kubernetes.io/service-account-token\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: inference-gateway-sa-metrics-reader-secret-read\nrules:\n- resources:\n  - secrets\n  apiGroups: [\"\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n  resourceNames: [\"inference-gateway-sa-metrics-reader-secret\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: gmp-system:collector:inference-gateway-sa-metrics-reader-secret-read\n  namespace: default\nroleRef:\n  name: inference-gateway-sa-metrics-reader-secret-read\n  kind: ClusterRole\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- name: collector\n  namespace: gmp-system\n  kind: ServiceAccount\nEOF\n```"}
{"text": "### Human: According to the quickstart guide, what command deploys the HealthCheckPolicy for a GKE-based gateway? ### Assistant: The quickstart guide provides a specific manifest for the HealthCheckPolicy. You can deploy it with the command:\n```bash\nkubectl apply -f [https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/healthcheck.yaml](https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/gke/healthcheck.yaml)\n```"}
{"text": "### Human: How do I confirm that an HTTPRoute has been accepted and its references resolved? ### Assistant: You can inspect the status of the HTTPRoute by retrieving its full YAML definition. A successful route will have status conditions showing `Accepted=True` and `ResolvedRefs=True`. Use this command:\n```bash\nkubectl get httproute llm-route -o yaml\n```"}
{"text": "### Human: What command would I use to deploy a CPU-based model server from the Inference Gateway quickstart guide? ### Assistant: To deploy the sample CPU-based model server (which uses the model Qwen/Qwen2.5-1.5B-Instruct), you would apply the corresponding manifest with kubectl:\n```bash\nkubectl apply -f [https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/cpu-deployment.yaml](https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/cpu-deployment.yaml)\n```"}
{"text": "### Human: How do I uninstall the InferencePool, InferenceModel, and model server resources from the quickstart guide? ### Assistant: To clean up the InferencePool, InferenceModel, and related model server resources, you can run `kubectl delete` with the original manifests and the `--ignore-not-found` flag:\n```bash\nkubectl delete -f [https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/inferencepool-resources.yaml](https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/inferencepool-resources.yaml) --ignore-not-found\nkubectl delete -f [https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/inferencemodel.yaml](https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/inferencemodel.yaml) --ignore-not-found\nkubectl delete -f [https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/cpu-deployment.yaml](https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/cpu-deployment.yaml) --ignore-not-found\nkubectl delete -f [https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/gpu-deployment.yaml](https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/vllm/gpu-deployment.yaml) --ignore-not-found\nkubectl delete secret hf-token --ignore-not-found\n```"}
{"text": "### Human: What is the command to uninstall the Gateway API Inference Extension CRDs using Kustomize? ### Assistant: To uninstall all the Custom Resource Definitions for the extension, you can use `kubectl delete` with the `-k` flag pointing to the `config/crd` directory of the repository:\n```bash\nkubectl delete -k [https://github.com/kubernetes-sigs/gateway-api-inference-extension/config/crd](https://github.com/kubernetes-sigs/gateway-api-inference-extension/config/crd) --ignore-not-found\n```"}
{"text": "### Human: What is the purpose of a LoRA adapter rollout as described in the guide? ### Assistant: A LoRA adapter rollout allows you to deploy new versions of LoRA adapters in phases without changing the base model or infrastructure. This lets you gradually test improvements, bug fixes, or new features with minimal service disruption, using techniques like traffic splitting and rollbacks for reliable deployments."}
{"text": "### Human: How do I load a new version of a LoRA adapter, for example 'food-review-2', using the LoRA syncer sidecar? ### Assistant: You can dynamically load a new adapter by editing the LoRA syncer ConfigMap to add the new adapter version to the `ensureExist.models` list. Use the command `kubectl edit configmap vllm-llama3-8b-instruct-adapters` and update the YAML to include the new model entry. This applies the change live without restarting the model servers."}
{"text": "### Human: Show me the ConfigMap YAML after adding a new 'food-review-2' adapter to an existing deployment. ### Assistant: Here is the YAML for the `vllm-llama3-8b-instruct-adapters` ConfigMap after adding the new `food-review-2` adapter under the `ensureExist` list:\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vllm-llama3-8b-instruct-adapters\ndata:\n  configmap.yaml: |\n    vLLMLoRAConfig:\n      name: vllm-llama3-8b-instruct-adapters\n      port: 8000\n      defaultBaseModel: meta-llama/Llama-3.1-B-Instruct\n      ensureExist:\n        models:\n        - id: food-review-1\n          source: Kawon/llama3.1-food-finetune_v14_r8\n        - id: food-review-2\n          source: Kawon/llama3.1-food-finetune_v14_r8\n```"}
{"text": "### Human: How do I perform a canary rollout by splitting traffic 90/10 between two LoRA adapter versions? ### Assistant: To split traffic, you modify the `InferenceModel` resource to include both adapter versions in the `targetModels` list, assigning a weight to each. For a 90/10 split, you would use the command `kubectl edit inferencemodel food-review` and set the weights accordingly."}
{"text": "### Human: Provide the InferenceModel YAML for splitting traffic, sending 90% to 'food-review-1' and 10% to 'food-review-2'. ### Assistant: Here is the `InferenceModel` manifest for a 90/10 traffic split between the two adapter versions:\n```yaml\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: food-review\nspec:\n  modelName: food-review\n  criticality: Standard\n  poolRef:\n    name: vllm-llama3-8b-instruct\n  targetModels:\n  - name: food-review-1\n    weight: 90\n  - name: food-review-2\n    weight: 10\n```"}
{"text": "### Human: How do I complete a LoRA adapter rollout and direct 100% of traffic to the new version? ### Assistant: To finish the rollout, you edit the `InferenceModel` again and modify the `targetModels` list to give the new version (`food-review-2`) a weight of 100, removing the old version from the list."}
{"text": "### Human: Show me the InferenceModel YAML to direct 100% of traffic to the 'food-review-2' adapter. ### Assistant: This `InferenceModel` manifest configuration directs all traffic for the 'food-review' model to the `food-review-2` adapter:\n```yaml\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: food-review\nspec:\n  modelName: food-review\n  criticality: Standard\n  poolRef:\n    name: vllm-llama3-8b-instruct\n  targetModels:\n  - name: food-review-2\n    weight: 100\n```"}
{"text": "### Human: How do I unload an old LoRA adapter version from the model servers after a rollout is complete? ### Assistant: To unload an old adapter, you update the LoRA syncer ConfigMap. You move the old adapter's entry from the `ensureExist` list to a new `ensureNotExist` list. This signals the syncer to remove the adapter from the running model servers."}
{"text": "### Human: What does the LoRA syncer ConfigMap look like when unloading 'food-review-1' and keeping 'food-review-2'? ### Assistant: After the rollout is complete, the ConfigMap should be updated to have the old version in `ensureNotExist` and the new version in `ensureExist`, like this:\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vllm-llama3-8b-instruct-adapters\ndata:\n  configmap.yaml: |\n    vLLMLoRAConfig:\n      name: vllm-llama3-8b-instruct-adapters\n      port: 8000\n      defaultBaseModel: meta-llama/Llama-3.1-B-Instruct\n      ensureExist:\n        models:\n        - id: food-review-2\n          source: Kawon/llama3.1-food-finetune_v14_r8\n      ensureNotExist:\n        models:\n        - id: food-review-1\n          source: Kawon/llama3.1-food-finetune_v14_r8\n```"}
{"text": "### Human: What is the purpose of an InferencePool rollout? ### Assistant: An InferencePool rollout is a technique for performing incremental updates to your inference infrastructure, such as updating nodes, base models, or model servers. It allows you to gradually introduce changes, monitor their impact, and revert to a previous state if necessary, all with minimal service disruption."}
{"text": "### Human: What are the main use cases for an InferencePool rollout? ### Assistant: The main use cases are: 1. Node(compute, accelerator) update rollout to safely migrate workloads to new hardware. 2. Base model rollout to upgrade to a new base LLM while retaining LoRA adapter compatibility. 3. Model server framework rollout to deploy new versions of serving frameworks like vLLM."}
{"text": "### Human: What are the high-level steps to perform an InferencePool rollout? ### Assistant: The process involves four main steps: 1. Deploy the new infrastructure by creating a new InferencePool. 2. Configure traffic splitting using an HTTPRoute to direct a portion of traffic to the new pool. 3. Maintain the existing InferenceModel configuration to ensure uniform behavior. 4. Retain the original InferencePool during the rollout to allow for a quick rollback if needed."}
{"text": "### Human: How do I deploy a new InferencePool and all its related resources for a node update rollout? ### Assistant: You can deploy the new infrastructure, including the new Deployment, ConfigMap, InferenceModel, InferencePool, Service, and Endpoint Picker Extension (EPP) Deployment, by applying a single multi-document YAML manifest like this:\n```bash\nkubectl apply -f - <<EOF\n---\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: food-review-new\nspec:\n  modelName: food-review\n  criticality: Standard\n  poolRef:\n    name: vllm-llama3-8b-instruct-new\n  targetModels:\n    - name: food-review-1\n      weight: 100\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vllm-llama3-8b-instruct-new\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: vllm-llama3-8b-instruct-new\n  template:\n    metadata:\n      labels:\n        app: vllm-llama3-8b-instruct-new\n    spec:\n      containers:\n        - name: vllm\n          image: \"vllm/vllm-openai:latest\"\n          imagePullPolicy: Always\n          command: [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n          args:\n            - \"--model\"\n            - \"meta-llama/Llama-3.1-8B-Instruct\"\n            - \"--tensor-parallel-size\"\n            - \"1\"\n            - \"--port\"\n            - \"8000\"\n            - \"--max-num-seq\"\n            - \"1024\"\n            - \"--compilation-config\"\n            - \"3\"\n            - \"--enable-lora\"\n            - \"--max-loras\"\n            - \"2\"\n            - \"--max-lora-rank\"\n            - \"8\"\n            - \"--max-cpu-loras\"\n            - \"12\"\n          env:\n            - name: VLLM_USE_V1\n              value: \"1\"\n            - name: PORT\n              value: \"8000\"\n            - name: HUGGING_FACE_HUB_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: hf-token\n                  key: token\n            - name: VLLM_ALLOW_RUNTIME_LORA_UPDATING\n              value: \"true\"\n          ports:\n            - containerPort: 8000\n              name: http\n              protocol: TCP\n          lifecycle:\n            preStop:\n              sleep:\n                seconds: 30\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n            periodSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            timeoutSeconds: 1\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n            periodSeconds: 1\n            successThreshold: 1\n            failureThreshold: 1\n            timeoutSeconds: 1\n          startupProbe:\n            failureThreshold: 600\n            initialDelaySeconds: 2\n            periodSeconds: 1\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n          resources:\n            limits:\n              [nvidia.com/gpu](https://nvidia.com/gpu): 1\n            requests:\n              [nvidia.com/gpu](https://nvidia.com/gpu): 1\n          volumeMounts:\n            - mountPath: /data\n              name: data\n            - mountPath: /dev/shm\n              name: shm\n            - name: adapters\n              mountPath: \"/adapters\"\n      initContainers:\n        - name: lora-adapter-syncer\n          tty: true\n          stdin: true\n          image: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension/lora-syncer:main\n          restartPolicy: Always\n          imagePullPolicy: Always\n          env:\n            - name: DYNAMIC_LORA_ROLLOUT_CONFIG\n              value: \"/config/configmap.yaml\"\n          volumeMounts:\n            - name: config-volume\n              mountPath:  /config\n      restartPolicy: Always\n      enableServiceLinks: false\n      terminationGracePeriodSeconds: 130\n      nodeSelector:\n        [cloud.google.com/gke-accelerator](https://cloud.google.com/gke-accelerator): \"nvidia-h100-80gb\"\n      volumes:\n        - name: data\n          emptyDir: {}\n        - name: shm\n          emptyDir:\n            medium: Memory\n        - name: adapters\n          emptyDir: {}\n        - name: config-volume\n          configMap:\n            name: vllm-llama3-8b-instruct-adapters-new\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vllm-llama3-8b-instruct-adapters-new\ndata:\n  configmap.yaml: |\n    vLLMLoRAConfig:\n      name: vllm-llama3-8b-instruct-adapters-new\n      port: 8000\n      defaultBaseModel: meta-llama/Llama-3.1-8B-Instruct\n      ensureExist:\n        models:\n        - id: food-review-1\n          source: Kawon/llama3.1-food-finetune_v14_r8\n---\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferencePool\nmetadata:\n  name: vllm-llama3-8b-instruct-new\nspec:\n  targetPortNumber: 8000\n  selector:\n    app: vllm-llama3-8b-instruct-new\n  extensionRef:\n    name: vllm-llama3-8b-instruct-epp-new\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vllm-llama3-8b-instruct-epp-new\n  namespace: default\nspec:\n  selector:\n    app: vllm-llama3-8b-instruct-epp-new\n  ports:\n    - protocol: TCP\n      port: 9002\n      targetPort: 9002\n      appProtocol: http2\n  type: ClusterIP\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vllm-llama3-8b-instruct-epp-new\n  namespace: default\n  labels:\n    app: vllm-llama3-8b-instruct-epp-new\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vllm-llama3-8b-instruct-epp-new\n  template:\n    metadata:\n      labels:\n        app: vllm-llama3-8b-instruct-epp-new\n    spec:\n      terminationGracePeriodSeconds: 130\n      containers:\n        - name: epp\n          image: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension/epp:main\n          imagePullPolicy: Always\n          args:\n            - -poolName\n            - \"vllm-llama3-8b-instruct-new\"\n            - \"-poolNamespace\"\n            - \"default\"\n            - -v\n            - \"4\"\n            - --zap-encoder\n            - \"json\"\n            - -grpcPort\n            - \"9002\"\n            - -grpcHealthPort\n            - \"9003\"\n          ports:\n            - containerPort: 9002\n            - containerPort: 9003\n            - name: metrics\n              containerPort: 9090\n          livenessProbe:\n            grpc:\n              port: 9003\n              service: inference-extension\n            initialDelaySeconds: 5\n            periodSeconds: 10\n          readinessProbe:\n            grpc:\n              port: 9003\n              service: inference-extension\n            initialDelaySeconds: 5\n            periodSeconds: 10\nEOF\n```"}
{"text": "### Human: How do I split traffic between an old and a new InferencePool for a canary deployment? ### Assistant: You can incrementally split traffic by editing your HTTPRoute resource and adding both InferencePools to the `backendRefs` list, assigning a specific `weight` to each one to control the percentage of traffic they receive. For example, use the command `kubectl edit httproute llm-route` to begin."}
{"text": "### Human: Provide the HTTPRoute YAML to split traffic 90% to an existing InferencePool and 10% to a new one. ### Assistant: This HTTPRoute manifest directs 90% of traffic to the `vllm-llama3-8b-instruct` InferencePool and 10% to the `vllm-llama3-8b-instruct-new` InferencePool.\n```yaml\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: llm-route\nspec:\n  parentRefs:\n    - group: gateway.networking.k8s.io\n      kind: Gateway\n      name: inference-gateway\n  rules:\n    - backendRefs:\n        - group: inference.networking.x-k8s.io\n          kind: InferencePool\n          name: vllm-llama3-8b-instruct\n          weight: 90\n        - group: inference.networking.x-k8s.io\n          kind: InferencePool\n          name: vllm-llama3-8b-instruct-new\n          weight: 10\n      matches:\n        - path:\n            type: PathPrefix\n            value: /\n```"}
{"text": "### Human: How do I finish an InferencePool rollout and direct 100% of traffic to the new pool? ### Assistant: To complete the rollout, edit the HTTPRoute again and change the `backendRefs` to direct 100% of the traffic to the new InferencePool by setting its weight to 100 and removing the old pool from the list."}
{"text": "### Human: Show the HTTPRoute YAML for directing 100% of traffic to the 'vllm-llama3-8b-instruct-new' InferencePool. ### Assistant: Here is the manifest to configure the HTTPRoute to send all traffic to the new InferencePool:\n```yaml\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: llm-route\nspec:\n  parentRefs:\n    - group: gateway.networking.k8s.io\n      kind: Gateway\n      name: inference-gateway\n  rules:\n    - backendRefs:\n        - group: inference.networking.x-k8s.io\n          kind: InferencePool\n          name: vllm-llama3-8b-instruct-new\n          weight: 100\n      matches:\n        - path:\n            type: PathPrefix\n            value: /\n```"}
{"text": "### Human: What is the full set of commands to delete the old version of an InferencePool and its related resources after a rollout? ### Assistant: After successfully rolling out to the new infrastructure, you can clean up the old resources with the following set of commands:\n```bash\nkubectl delete InferenceModel food-review --ignore-not-found\nkubectl delete Deployment vllm-llama3-8b-instruct --ignore-not-found\nkubectl delete ConfigMap vllm-llama3-8b-instruct-adapters --ignore-not-found\nkubectl delete InferencePool vllm-llama3-8b-instruct --ignore-not-found\nkubectl delete Deployment vllm-llama3-8b-instruct-epp --ignore-not-found\nkubectl delete Service vllm-llama3-8b-instruct-epp --ignore-not-found\n```"}
