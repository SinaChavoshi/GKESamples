apiVersion: kubeflow.org/v1
kind: TFJob
metadata:
  name: tf-16-dlrm-tfjob
spec:
  runPolicy:
    cleanPodPolicy: Running
    ttlSecondsAfterFinished: 600
  tfReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: ExitCode
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "false"
          labels:
            app: tfjob
        spec:
          containers:
            - name: tensorflow
              command:
                - bash
                - -c
                - |
                  TF_USE_LEGACY_KERAS=1 TPU_LOAD_LIBRARY=0 TF_XLA_FLAGS="--tf_mlir_enable_mlir_bridge=true \
                  --tf_xla_sparse_core_disable_table_stacking=true \
                  --tf_mlir_enable_convert_control_to_data_outputs_pass=true \
                  --tf_mlir_enable_merge_control_flow_pass=true" python3 -c "
                  import os

                  os.environ['TPU_LOAD_LIBRARY'] = '0'

                  import argparse
                  import random
                  import json
                  from typing import List
                  import numpy as np
                  import tensorflow as tf
                  import tensorflow_datasets as tfds
                  import tensorflow_recommenders as tfrs
                  from tensorflow_recommenders.layers.embedding import TPUEmbedding


                  # --- Reproducibility ---
                  SEED = 42
                  random.seed(SEED)
                  np.random.seed(SEED)
                  tf.random.set_seed(SEED)
                  os.environ['TF_DETERMINISTIC_OPS'] = '1'

                  # Constants
                  GCS_BUCKET = 'gs://chavoshi-checkpoints/'
                  CHECKPOINT_DIR = os.path.join(GCS_BUCKET, 'checkpoints-kevinmcw-0702')
                  PER_REPLICA_BATCH_SIZE = 16
                  MOVIE_VOCAB_SIZE = 2048
                  USER_VOCAB_SIZE = 2048
                  EMBED_DIM = 64
                  TRAIN_SIZE = 80_000
                  TEST_SIZE = 20_000
                  SHUFFLE_BUF = 100_000


                  endpoints = 'grpc://tf-16-dlrm-tfjob-worker-0.default.svc,grpc://tf-16-dlrm-tfjob-worker-1.default.svc,grpc://tf-16-dlrm-tfjob-worker-2.default.svc,grpc://tf-16-dlrm-tfjob-worker-3.default.svc'

                  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(endpoints)
                  tf.config.experimental_connect_to_cluster(resolver)
                  topology = tf.tpu.experimental.initialize_tpu_system(resolver)
                  hardware_feature = tf.tpu.experimental.HardwareFeature(resolver.tpu_hardware_feature)
                  if hardware_feature.embedding_feature == tf.tpu.experimental.HardwareFeature.EmbeddingFeature.V2:
                      tpu_system_metadata = resolver.get_tpu_system_metadata()
                      device_assignment = tf.tpu.experimental.DeviceAssignment.build(
                          topology, num_replicas=tpu_system_metadata.num_hosts
                      )
                      print(device_assignment.topology.device_coordinates)
                  else:
                      device_assignment = None

                  strategy = tf.distribute.TPUStrategy(resolver)

                  ratings = tfds.load(
                      'movielens/100k-ratings', split='train', data_dir=GCS_BUCKET, shuffle_files=False).map(
                          lambda x: {
                              'movie_id': tf.strings.to_number(x['movie_id']),
                              'user_id': tf.strings.to_number(x['user_id']),
                              'user_rating': x['user_rating']
                          })

                  def prepare_dataset(split):
                    ds = ratings
                    if split == 'train':
                      ds = ds.take(TRAIN_SIZE)
                    else:
                      ds = ds.skip(TRAIN_SIZE).take(TEST_SIZE)
                    ds = ds.batch(PER_REPLICA_BATCH_SIZE * strategy.num_replicas_in_sync, drop_remainder=True).cache()
                    options = tf.data.Options()
                    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
                    return ds.with_options(options)

                  train_ds = prepare_dataset('train')
                  test_ds = prepare_dataset('test')
                  dist_train_ds = strategy.experimental_distribute_dataset(
                      train_ds, options=tf.distribute.InputOptions(experimental_fetch_to_device=False))
                  dist_test_ds = strategy.experimental_distribute_dataset(
                      test_ds, options=tf.distribute.InputOptions(experimental_fetch_to_device=False))

                  optimizer = tf.keras.optimizers.legacy.Adagrad(learning_rate=0.1)

                  user_table = tf.tpu.experimental.embedding.TableConfig(vocabulary_size=USER_VOCAB_SIZE, dim=EMBED_DIM, name='user_id')
                  movie_table = tf.tpu.experimental.embedding.TableConfig(
                      vocabulary_size=MOVIE_VOCAB_SIZE, dim=EMBED_DIM, name='movie_id')
                  feature_config = {
                      'user_id':
                          tf.tpu.experimental.embedding.FeatureConfig(
                              table=user_table, output_shape=[PER_REPLICA_BATCH_SIZE], name='user_id'),
                      'movie_id':
                          tf.tpu.experimental.embedding.FeatureConfig(
                              table=movie_table, output_shape=[PER_REPLICA_BATCH_SIZE], name='movie_id'),
                  }

                  class EmbeddingModel(tfrs.models.Model):
                    def __init__(self):
                      super().__init__()
                      self.embedding_layer = TPUEmbedding(
                          feature_config,
                          optimizer,
                          batch_size=PER_REPLICA_BATCH_SIZE,
                          pipeline_execution_with_tensor_core=False,
                          sparse_core_embedding_config=tf.tpu.experimental.embedding.SparseCoreEmbeddingConfig(
                              disable_table_stacking=True,
                              allow_id_dropping=False,
                              initialize_tables_on_host=True,
                              max_ids_per_chip_per_sample=128,
                          ))
                      self.ratings = tf.keras.Sequential([
                          tf.keras.layers.Dense(256, activation='relu'),
                          tf.keras.layers.Dense(64, activation='relu'),
                          tf.keras.layers.Dense(1)
                      ])
                      self.task = tfrs.tasks.Ranking(
                          loss=tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE),
                          metrics=[tf.keras.metrics.RootMeanSquaredError()])

                    def compute_loss(self, features, training=False):
                      emb = self.embedding_layer({'user_id': features['user_id'], 'movie_id': features['movie_id']})
                      preds = self.ratings(tf.concat([emb['user_id'], emb['movie_id']], axis=1))
                      return tf.reduce_sum(self.task(labels=features['user_rating'], predictions=preds)) \
                            * (1 / (PER_REPLICA_BATCH_SIZE * strategy.num_replicas_in_sync))

                  if __name__ == '__main__':
                    parser = argparse.ArgumentParser()
                    parser.add_argument('--training', action='store_true', help='Run training before evaluation')
                    args = parser.parse_args()

                    for i in range(1):
                      print(f'Iteration {i}')
                      with strategy.scope():
                        model = EmbeddingModel()
                        model.compile(optimizer=optimizer)

                        checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
                        manager = tf.train.CheckpointManager(checkpoint, CHECKPOINT_DIR, max_to_keep=3)
                        if manager.latest_checkpoint:
                          checkpoint.restore(manager.latest_checkpoint).assert_existing_objects_matched()
                          print(f'Restored from {manager.latest_checkpoint}')
                        else:
                          print('Initializing from scratch.')
                        
                        print('Starting training...')
                        model.fit(dist_train_ds, steps_per_epoch=10, epochs=10)
                        ckpt_path = manager.save()
                        print(f'Checkpoint saved at: {ckpt_path}')
                        
                        print('Starting evaluation after training...')
                        results = model.evaluate(dist_test_ds, steps=TEST_SIZE // (PER_REPLICA_BATCH_SIZE * strategy.num_replicas_in_sync))
                        print(f'Final evaluation results (loss, RMSE): {results}')
                  "
              image: us-central2-docker.pkg.dev/tpu-vm-gke-testing/tpu-repo/dlrm-master:latest
              imagePullPolicy: Always
              env:
                - name: TF_JOB_PACKAGE_URIS
                  value: ""
              resources:
                limits: {}
                requests: {}
    Worker:
      replicas: 4
      restartPolicy: ExitCode
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "false"
          labels:
            app: tfjob
        spec:
          nodeSelector:
            cloud.google.com/gke-nodepool: v6e-16
            cloud.google.com/gke-tpu-accelerator: tpu-v6e-slice
            cloud.google.com/gke-tpu-topology: 4x4
          tolerations:
            - effect: NoSchedule
              key: dedicated
              operator: Equal
              value: tfjob
            - effect: NoSchedule
              key: google.com/tpu
              operator: Equal
              value: present
          containers:
            - name: tensorflow
              command:
                - bash
                - -c
                - |
                  tpu_tfjob_worker.sh
              env:
                - name: PYTHONUNBUFFERED
                  value: "1"
                - name: TPU_WORKER_HOSTNAMES
                  value: "tf-16-dlrm-tfjob-worker-0.default.svc,tf-16-dlrm-tfjob-worker-1.default.svc,tf-16-dlrm-tfjob-worker-2.default.svc,tf-16-dlrm-tfjob-worker-3.default.svc"
                - name: HOSTNAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.labels['training.kubeflow.org/replica-index']
              image: us-central2-docker.pkg.dev/tpu-vm-gke-testing/tpu-repo/dlrm-worker:latest
              imagePullPolicy: Always
              resources:
                limits:
                  google.com/tpu: "4"
                requests:
                  google.com/tpu: "4"
